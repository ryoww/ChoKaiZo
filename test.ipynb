{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import customtkinter as ctk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# 外観モードとテーマの設定（任意）\n",
    "ctk.set_appearance_mode(\"light\")    # \"light\"または\"dark\"\n",
    "ctk.set_default_color_theme(\"blue\")   # カラーテーマ\n",
    "\n",
    "# メインウィンドウの作成\n",
    "app = ctk.CTk()\n",
    "app.geometry(\"400x200\")\n",
    "app.title(\"ディレクトリ選択のデモ\")\n",
    "\n",
    "def select_directory():\n",
    "    # ディレクトリ選択ダイアログを表示\n",
    "    directory_path = filedialog.askdirectory(title=\"ディレクトリを選択してください\")\n",
    "    if directory_path:\n",
    "        # 選択されたパスをラベルに表示\n",
    "        label.configure(text=f\"選択されたディレクトリ:\\n{directory_path}\")\n",
    "    else:\n",
    "        label.configure(text=\"ディレクトリが選択されませんでした\")\n",
    "\n",
    "# ボタンウィジェット：クリックでディレクトリ選択ダイアログを表示\n",
    "select_button = ctk.CTkButton(app, text=\"ディレクトリを選択\", command=select_directory)\n",
    "select_button.pack(pady=20)\n",
    "\n",
    "# 選択結果を表示するラベル\n",
    "label = ctk.CTkLabel(app, text=\"まだディレクトリが選択されていません\")\n",
    "label.pack(pady=20)\n",
    "\n",
    "# メインループ開始\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "GPU device count: 1\n",
      "Current CUDA device index: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA が利用可能かどうかを確認\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "\n",
    "# 利用可能な GPU の数を確認\n",
    "print(\"GPU device count:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # 現在の CUDA デバイスのインデックスを取得\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(\"Current CUDA device index:\", current_device)\n",
    "    # 現在のデバイスの名前を取得\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(current_device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrrdbnet_arch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RRDBNet\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryosh\\.virtualenvs\\ChoKaiZo-0j5vMKI8\\Lib\\site-packages\\basicsr\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# https://github.com/xinntao/BasicSR\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ryosh\\.virtualenvs\\ChoKaiZo-0j5vMKI8\\Lib\\site-packages\\basicsr\\data\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m [osp\u001b[38;5;241m.\u001b[39msplitext(osp\u001b[38;5;241m.\u001b[39mbasename(v))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scandir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset.py\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# import all the dataset modules\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m _dataset_modules \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbasicsr.data.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_filenames\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_dataset\u001b[39m(dataset_opt):\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build dataset from options.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m            type (str): Dataset type.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryosh\\.virtualenvs\\ChoKaiZo-0j5vMKI8\\Lib\\site-packages\\basicsr\\data\\__init__.py:22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m dataset_filenames \u001b[38;5;241m=\u001b[39m [osp\u001b[38;5;241m.\u001b[39msplitext(osp\u001b[38;5;241m.\u001b[39mbasename(v))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scandir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset.py\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# import all the dataset modules\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m _dataset_modules \u001b[38;5;241m=\u001b[39m [\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbasicsr.data.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m dataset_filenames]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_dataset\u001b[39m(dataset_opt):\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build dataset from options.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m            type (str): Dataset type.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryosh\\.virtualenvs\\ChoKaiZo-0j5vMKI8\\Lib\\site-packages\\basicsr\\data\\realesrgan_dataset.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data \u001b[38;5;28;01mas\u001b[39;00m data\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdegradations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m circular_lowpass_kernel, random_mixed_kernels\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m augment\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbasicsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileClient, get_root_logger, imfrombytes, img2tensor\n",
      "File \u001b[1;32mc:\\Users\\ryosh\\.virtualenvs\\ChoKaiZo-0j5vMKI8\\Lib\\site-packages\\basicsr\\data\\degradations.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m special\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multivariate_normal\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rgb_to_grayscale\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# --------------------------- blur kernels --------------------------- #\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# --------------------------- util functions --------------------------- #\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msigma_matrix2\u001b[39m(sig_x, sig_y, theta):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def tile_inference(model, input_tensor, tile_size, tile_pad, scale):\n",
    "    \"\"\"\n",
    "    入力テンソル（[1, C, H, W]）に対して、タイル処理を行いモデル推論を適用する関数\n",
    "    Args:\n",
    "      model: 推論に使用するモデル（GPU上に配置済み）\n",
    "      input_tensor: 入力画像テンソル [1, C, H, W]\n",
    "      tile_size: タイルサイズ（例: 256）\n",
    "      tile_pad: タイル間のオーバーラップ幅（例: 10）\n",
    "      scale: 拡大率（例: 4）\n",
    "    Returns:\n",
    "      出力テンソル [1, C, H*scale, W*scale]\n",
    "    \"\"\"\n",
    "    _, C, H, W = input_tensor.shape\n",
    "    output = torch.zeros(1, C, H * scale, W * scale, device=input_tensor.device)\n",
    "    h_tiles = math.ceil(H / tile_size)\n",
    "    w_tiles = math.ceil(W / tile_size)\n",
    "    \n",
    "    for i in range(h_tiles):\n",
    "        for j in range(w_tiles):\n",
    "            # タイルの元の領域（paddingなし）\n",
    "            h_start = i * tile_size\n",
    "            h_end = min(h_start + tile_size, H)\n",
    "            w_start = j * tile_size\n",
    "            w_end = min(w_start + tile_size, W)\n",
    "            \n",
    "            # padding 分を含めた領域（ただし画像境界を超えないように）\n",
    "            h_start_pad = max(h_start - tile_pad, 0)\n",
    "            h_end_pad = min(h_end + tile_pad, H)\n",
    "            w_start_pad = max(w_start - tile_pad, 0)\n",
    "            w_end_pad = min(w_end + tile_pad, W)\n",
    "            \n",
    "            # 対象タイルを抽出\n",
    "            input_tile = input_tensor[:, :, h_start_pad:h_end_pad, w_start_pad:w_end_pad]\n",
    "            \n",
    "            # タイルに対して推論（with torch.no_grad() で勾配計算なし）\n",
    "            with torch.no_grad():\n",
    "                output_tile = model(input_tile)\n",
    "            \n",
    "            # 入力で追加した padding に対応する出力側のオフセット（scale 倍）\n",
    "            h_valid_start = (h_start - h_start_pad) * scale\n",
    "            h_valid_end = h_valid_start + (h_end - h_start) * scale\n",
    "            w_valid_start = (w_start - w_start_pad) * scale\n",
    "            w_valid_end = w_valid_start + (w_end - w_start) * scale\n",
    "            \n",
    "            valid_output = output_tile[:, :, h_valid_start:h_valid_end, w_valid_start:w_valid_end]\n",
    "            \n",
    "            # 出力画像内の配置位置（scale 倍）\n",
    "            H_out_start = h_start * scale\n",
    "            H_out_end = h_end * scale\n",
    "            W_out_start = w_start * scale\n",
    "            W_out_end = w_end * scale\n",
    "            \n",
    "            output[:, :, H_out_start:H_out_end, W_out_start:W_out_end] = valid_output\n",
    "\n",
    "    return output\n",
    "\n",
    "# 1. モデルの定義（RealESRGAN_x4plus用のRRDBNet）\n",
    "scale = 16\n",
    "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64,\n",
    "                num_block=23, num_grow_ch=32, scale=scale)\n",
    "\n",
    "# 2. 学習済み .pth ファイルのロード\n",
    "checkpoint = torch.load('./models/RealESRGAN_x4plus.pth', map_location='cuda')\n",
    "if 'params_ema' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['params_ema'])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "# 3. 推論モードに設定し、CUDA上かつ半精度で動作させる\n",
    "model.eval()\n",
    "model = model.cuda().half()\n",
    "\n",
    "# 4. 入力画像の読み込みと前処理\n",
    "input_image = Image.open('./assets/input.jpg').convert('RGB')\n",
    "img_np = np.array(input_image).astype('float32') / 255.0\n",
    "img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0).cuda().half()\n",
    "\n",
    "# 5. タイル処理を用いた推論\n",
    "tile_size = 256  # タイルサイズ（適宜調整）\n",
    "tile_pad = 10    # タイルのパディング幅（オーバーラップ）\n",
    "with torch.no_grad():\n",
    "    output_tensor = tile_inference(model, img_tensor, tile_size, tile_pad, scale)\n",
    "\n",
    "# 6. 結果の後処理と保存\n",
    "output_tensor = output_tensor.squeeze(0).float().cpu().clamp_(0, 1)\n",
    "output_image = Image.fromarray((output_tensor.permute(1, 2, 0).numpy() * 255).astype('uint8'))\n",
    "output_image.save('./assets/output_image.jpg')\n",
    "\n",
    "print(\"超解像処理が完了しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChoKaiZo-0j5vMKI8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
